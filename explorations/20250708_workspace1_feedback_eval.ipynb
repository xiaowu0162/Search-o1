{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d671396-ceb0-4419-a86a-a9038a4908c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "import backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f64205-f87b-4f53-8bb1-15848d81e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='Qwen/QwQ-32B', created=1752006066, object='model', owned_by='vllm', root='Qwen/QwQ-32B', parent=None, max_model_len=40960, permission=[{'id': 'modelperm-48f1809e946f4cc9a67c32c1939d4c19', 'object': 'model_permission', 'created': 1752006066, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')\n"
     ]
    }
   ],
   "source": [
    "port = 8003\n",
    "model_name = 'Qwen/QwQ-32B'\n",
    "\n",
    "OPENAI_REQUEST_TIMEOUT = 60*60*24 \n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"EMPTY\", timeout=OPENAI_REQUEST_TIMEOUT)\n",
    "print(client.models.list())\n",
    "\n",
    "# @backoff.on_exception(backoff.constant, Exception, interval=5)\n",
    "def run_chat_completion_with_backoff(client, **kwargs):\n",
    "    return client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.constant, Exception, interval=5)\n",
    "def run_generate_with_backoff(client, **kwargs):\n",
    "    return client.completions.create(**kwargs)\n",
    "\n",
    "# load direct pred logs\n",
    "def load_direct_pred_logs(task):\n",
    "    if task == 'gpqa':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/gpqa.qwq.direct/diamond.7.1,20:8.json'))\n",
    "    elif task == 'aime':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/aime.qwq.direct/test.7.1,19:42.json'))\n",
    "    elif task == 'amc':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/amc.qwq.direct/test.7.1,19:24.json'))\n",
    "    elif task == 'math500':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/math500.qwq.direct/test.7.1,20:17.json'))\n",
    "    elif task == 'livecode':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/livecode.qwq.direct/test_1to4.7.1,22:30.json'))\n",
    "    elif task == 'bamboogle':\n",
    "        logs = json.load(open('/fsx-comem/diwu0162/Search-o1/outputs/runs.qa/bamboogle.qwq.direct/test.7.1,16:22.json'))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return logs\n",
    "\n",
    "def segment_thoughts_v1(x):\n",
    "    return x.strip().split('\\n\\n')\n",
    "\n",
    "def segment_thoughts_v2(x):\n",
    "    # note: excluding things like \"so\" \"therefore\", \"but\", \"let me\" \n",
    "    reasoning_word_list = [\n",
    "        'okay', 'hmm', 'wait', 'but wait', 'oh wait', 'no wait', 'no, wait', 'but let me', 'but actually', 'alternatively', \n",
    "        'now', 'the question', 'ah', 'oh', 'next', 'another angle', 'another approach', 'also', 'hold on', 'looking it up', \n",
    "        'another point', 'I don\\'t think', 'perhaps I', 'putting this together', 'Putting it all together', 'i\\'m', 'but i\\'m',   \n",
    "        'let me think again', 'I don\\'t see', 'maybe I', 'alternative', \"I wonder if\", \"another way\", 'an alternative', \n",
    "    ]\n",
    "    prefix_len = max([len(x) for x in reasoning_word_list])\n",
    "    newline_segmented_thoughts = segment_thoughts_v1(x)\n",
    "    final_thoughts = []\n",
    "    for t in newline_segmented_thoughts:\n",
    "        t_lower = t.lower()\n",
    "        is_segment_start = False\n",
    "        for r_w in reasoning_word_list:\n",
    "            if t_lower.startswith(r_w.lower()):\n",
    "                is_segment_start = True\n",
    "                break\n",
    "        if is_segment_start or not final_thoughts:\n",
    "            final_thoughts.append(t)\n",
    "        else:\n",
    "            final_thoughts[-1] += '\\n\\n' + t\n",
    "    return final_thoughts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3ee59c-9abf-48d7-92aa-8c17378a21d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# evaluation helper (right now only supporting choice, qa, and math)\n",
    "\n",
    "def _fix_fracs(string):\n",
    "    substrs = string.split(\"\\\\frac\")\n",
    "    new_str = substrs[0]\n",
    "    if len(substrs) > 1:\n",
    "        substrs = substrs[1:]\n",
    "        for substr in substrs:\n",
    "            new_str += \"\\\\frac\"\n",
    "            if substr[0] == \"{\":\n",
    "                new_str += substr\n",
    "            else:\n",
    "                try:\n",
    "                    assert len(substr) >= 2\n",
    "                except:\n",
    "                    return string\n",
    "                a = substr[0]\n",
    "                b = substr[1]\n",
    "                if b != \"{\":\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
    "                else:\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}\" + b\n",
    "    string = new_str\n",
    "    return string\n",
    "\n",
    "def _fix_a_slash_b(string):\n",
    "    if len(string.split(\"/\")) != 2:\n",
    "        return string\n",
    "    a = string.split(\"/\")[0]\n",
    "    b = string.split(\"/\")[1]\n",
    "    try:\n",
    "        a = int(a)\n",
    "        b = int(b)\n",
    "        assert string == \"{}/{}\".format(a, b)\n",
    "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
    "        return new_string\n",
    "    except:\n",
    "        return string\n",
    "\n",
    "def _remove_right_units(string):\n",
    "    # \"\\\\text{ \" only ever occurs (at least in the val set) when describing units\n",
    "    if \"\\\\text{ \" in string:\n",
    "        splits = string.split(\"\\\\text{ \")\n",
    "        assert len(splits) == 2\n",
    "        return splits[0]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def _fix_sqrt(string):\n",
    "    if \"\\\\sqrt\" not in string:\n",
    "        return string\n",
    "    splits = string.split(\"\\\\sqrt\")\n",
    "    new_string = splits[0] \n",
    "    for split in splits[1:]:\n",
    "        if split[0] != \"{\":\n",
    "            a = split[0]\n",
    "            new_substr = \"\\\\sqrt{\" + a + \"}\" + split[1:]\n",
    "        else:\n",
    "            new_substr = \"\\\\sqrt\" + split\n",
    "        new_string += new_substr\n",
    "    return new_string\n",
    "\n",
    "def _strip_string(string):\n",
    "    # linebreaks  \n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "    #print(string)\n",
    "\n",
    "    # remove inverse spaces\n",
    "    string = string.replace(\"\\\\!\", \"\")\n",
    "    #print(string)\n",
    "\n",
    "    # replace \\\\ with \\\n",
    "    string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    #print(string)\n",
    "\n",
    "    # replace tfrac and dfrac with frac\n",
    "    string = string.replace(\"tfrac\", \"frac\")\n",
    "    string = string.replace(\"dfrac\", \"frac\")\n",
    "    #print(string)\n",
    "\n",
    "    # remove \\left and \\right\n",
    "    string = string.replace(\"\\\\left\", \"\")\n",
    "    string = string.replace(\"\\\\right\", \"\")\n",
    "    #print(string)\n",
    "    \n",
    "    # Remove circ (degrees)\n",
    "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
    "    string = string.replace(\"^\\\\circ\", \"\")\n",
    "\n",
    "    # remove dollar signs\n",
    "    string = string.replace(\"\\\\$\", \"\")\n",
    "    \n",
    "    # remove units (on the right)\n",
    "    string = _remove_right_units(string)\n",
    "\n",
    "    # remove percentage\n",
    "    string = string.replace(\"\\\\%\", \"\")\n",
    "    string = string.replace(\"\\%\", \"\")\n",
    "\n",
    "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
    "    string = string.replace(\" .\", \" 0.\")\n",
    "    string = string.replace(\"{.\", \"{0.\")\n",
    "    # if empty, return empty string\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    if string[0] == \".\":\n",
    "        string = \"0\" + string\n",
    "\n",
    "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
    "    if len(string.split(\"=\")) == 2:\n",
    "        if len(string.split(\"=\")[0]) <= 2:\n",
    "            string = string.split(\"=\")[1]\n",
    "\n",
    "    # fix sqrt3 --> sqrt{3}\n",
    "    string = _fix_sqrt(string)\n",
    "\n",
    "    # remove spaces\n",
    "    string = string.replace(\" \", \"\")\n",
    "\n",
    "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc. Even works with \\frac1{72} (but not \\frac{72}1). Also does a/b --> \\\\frac{a}{b}\n",
    "    string = _fix_fracs(string)\n",
    "\n",
    "    # manually change 0.5 --> \\frac{1}{2}\n",
    "    if string == \"0.5\":\n",
    "        string = \"\\\\frac{1}{2}\"\n",
    "\n",
    "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n",
    "    string = _fix_a_slash_b(string)\n",
    "\n",
    "    return string\n",
    "\n",
    "def is_equiv(str1, str2, verbose=False):\n",
    "    if str1 is None and str2 is None:\n",
    "        print(\"WARNING: Both None\")\n",
    "        return True\n",
    "    if str1 is None or str2 is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ss1 = _strip_string(str1)\n",
    "        ss2 = _strip_string(str2)\n",
    "        if verbose:\n",
    "            print(ss1, ss2)\n",
    "        return ss1 == ss2\n",
    "    except:\n",
    "        return str1 == str2\n",
    "        \n",
    "def extract_answer_fn(output, mode='qa', extract_answer=False):\n",
    "    if extract_answer == False and mode not in ['infogen', 'summary', 'research']:\n",
    "        if mode == 'qa':\n",
    "            return output.strip()\n",
    "        pred_answer_lines = output.replace(\"\\n\\n\", \"\\n\").strip().split('\\n')\n",
    "        pred_answer = '\\n'.join(pred_answer_lines[-3:])\n",
    "        return pred_answer\n",
    "    extracted_text = ''\n",
    "    if mode == 'codegen':\n",
    "        pattern = r'```python\\s*(.*?)\\s*```'  # Extract the code between ```python and ```\n",
    "        matches = re.findall(pattern, output, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            extracted_text = matches[-1].strip()  # Take the last match\n",
    "    elif mode in ['infogen', 'summary', 'research']:\n",
    "        pattern_info = \"**Final Information\"\n",
    "        if \"</think>\\n\" in output:\n",
    "            extracted_text = output.split(\"</think>\\n\")[-1].split(\"<|begin_click_link|>\")[0].replace(pattern_info, \"\").strip(':**').strip('\\n').strip(\"```\").strip()  # 提取</think>后面的内容\n",
    "            if mode == 'infogen':\n",
    "                extracted_text = '\\n'.join(extracted_text.replace(\"\\n\\n\", \"\\n\").split('\\n')[:5])  # 只保留前5行\n",
    "        elif pattern_info in output:\n",
    "            extracted_text = output.split(pattern_info)[-1].split(\"<|begin_click_link|>\")[0].strip('\\n').strip(':**').strip(\"```\").strip()  # 提取**Final Information**后面的内容\n",
    "            if mode == 'infogen':\n",
    "                extracted_text = '\\n'.join(extracted_text.replace(\"\\n\\n\", \"\\n\").split('\\n')[:5])  # 只保留前5行\n",
    "        else:\n",
    "            # extracted_text = \"No helpful information found.\"\n",
    "            extracted_text = '\\n'.join(output.strip().replace(\"</think>\\n\", \"\").replace(\"\\n\\n\", \"\\n\").split('\\n')[-5:])  # 若没提取到，只保留最后5行\n",
    "        if mode == 'research':\n",
    "            extracted_text = extracted_text[:6000]\n",
    "        else:\n",
    "            extracted_text = extracted_text[:2500]\n",
    "    elif mode in ['math', 'choose', 'qa']:\n",
    "        pattern = r'\\\\boxed\\{(.*)\\}'\n",
    "        matches = re.findall(pattern, output)\n",
    "        if matches:\n",
    "            extracted_text = matches[-1]  # Take the last match\n",
    "        else:\n",
    "            pattern = 'ANSWER:'\n",
    "            if pattern in output:\n",
    "                extracted_text = output.split(pattern)[-1].strip('**').strip()\n",
    "        if mode in ['choose']:\n",
    "            inner_pattern = r'\\\\text\\{(.*)\\}'\n",
    "            inner_matches = re.findall(inner_pattern, extracted_text)\n",
    "            if inner_matches:\n",
    "                extracted_text = inner_matches[-1]  # Take the last match\n",
    "            extracted_text = extracted_text.strip(\"()\")\n",
    "    return extracted_text\n",
    "    \n",
    "def evaluate_predictions(output, labeled_answer, mode='math', use_llm=False, question=None, extract_answer=False):\n",
    "    final_metric = {\"is_valid_answer\": False, \"acc\": 0, \"em\": 0, \"f1\": 0, 'math_equal': 0, 'llm_equal': 0}\n",
    "    pred_answer = extract_answer_fn(output, mode=mode, extract_answer=extract_answer)\n",
    "    pred_answer_new = pred_answer\n",
    "    if pred_answer != '':\n",
    "        final_metric[\"is_valid_answer\"] = True\n",
    "    else:\n",
    "        # If no answer was extracted, keep only the last 3 lines\n",
    "        pred_answer_new = '\\n'.join(output.replace(\"\\n\\n\", \"\\n\").strip().split('\\n')[-5:])\n",
    "\n",
    "    if mode in ['qa']:\n",
    "        def normalize_answer_qa(s):\n",
    "            def remove_articles(text):\n",
    "                return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "            def white_space_fix(text):\n",
    "                return \" \".join(text.strip().split())\n",
    "            def remove_punc(text):\n",
    "                exclude = set(string.punctuation)\n",
    "                return \"\".join(ch for ch in text if ch not in exclude)\n",
    "            def lower(text):\n",
    "                return text.lower()\n",
    "            return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "        normalized_pred_answer = normalize_answer_qa(pred_answer_new)\n",
    "\n",
    "        for answer in labeled_answer:\n",
    "            normalized_ground_truth = normalize_answer_qa(answer)\n",
    "            em = int(normalized_pred_answer == normalized_ground_truth)\n",
    "            acc = int(normalized_ground_truth in normalized_pred_answer)\n",
    "\n",
    "            prediction_tokens = normalized_pred_answer.split()\n",
    "            ground_truth_tokens = normalized_ground_truth.split()\n",
    "            common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            if num_same == 0:\n",
    "                continue\n",
    "            precision = 1.0 * num_same / len(prediction_tokens)\n",
    "            recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            for k in [\"em\", \"acc\", \"f1\"]:\n",
    "                final_metric[k] = max(eval(k), final_metric[k])\n",
    "\n",
    "    elif mode in ['math', 'choose']:\n",
    "        def normalize_answer(text):\n",
    "            text = text.lower()\n",
    "            text = \" \".join(text.strip().split())\n",
    "            return text\n",
    "        normalized_pred_answer = normalize_answer(pred_answer_new)\n",
    "        normalized_ground_truth = normalize_answer(labeled_answer)\n",
    "\n",
    "        em = int(normalized_pred_answer == normalized_ground_truth)\n",
    "        acc = int(normalized_ground_truth in normalized_pred_answer)\n",
    "    \n",
    "        prediction_tokens = normalized_pred_answer.split()\n",
    "        ground_truth_tokens = normalized_ground_truth.split()\n",
    "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = 1.0 * num_same / len(prediction_tokens) if len(prediction_tokens) > 0 else 0\n",
    "            recall = 1.0 * num_same / len(ground_truth_tokens) if len(ground_truth_tokens) > 0 else 0\n",
    "            if (precision + recall) == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        final_metric[\"em\"] = em\n",
    "        final_metric[\"acc\"] = acc\n",
    "        final_metric[\"f1\"] = f1\n",
    "\n",
    "        final_metric[\"math_equal\"] = is_equiv(normalized_pred_answer, normalized_ground_truth)\n",
    "        \n",
    "        # Add LLM-based evaluation if requested\n",
    "        if use_llm and question is not None:\n",
    "            final_metric[\"llm_equal\"] = 0  # Will be updated in batch later\n",
    "\n",
    "    return final_metric, pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173bb6fd-a313-4be8-b676-804d03f83bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rollout log \n",
    "task = 'math500'\n",
    "if task in ['gpqa']:\n",
    "    eval_task_type = 'choose'\n",
    "    metric_name = 'acc'\n",
    "elif task in ['aime', 'amc', 'math500']:\n",
    "    eval_task_type = 'math'\n",
    "    metric_name = 'math_equal'\n",
    "# elif task in ['livecode']:\n",
    "#     eval_task_type = 'code'\n",
    "elif task in ['gaia', 'bamboogle']:\n",
    "    eval_task_type = 'qa'\n",
    "    metric_name = 'f1'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "assert eval_task_type in ['math', 'choose', 'qa']   # not supporting code here\n",
    "\n",
    "\n",
    "orig_pred_logs = load_direct_pred_logs(task)\n",
    "if task == 'math500':\n",
    "    orig_pred_logs_wrong = [x for x in orig_pred_logs if not x['Metrics']['math_equal']]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "rollout_logs = [json.loads(line) for line in open('/fsx-comem/diwu0162/Search-o1/explorations/perf_dicts_wrong_only_math500_20250708-1641.json').readlines()]\n",
    "# len(rollout_logs)\n",
    "# idx = 1\n",
    "# print([(x, np.mean(rollout_logs[idx]['perf_dict'][str(x)]['metrics'])) for x in range(len(rollout_logs[idx]['perf_dict']))])\n",
    "# print(json.dumps(rollout_logs[idx]['item'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b03bdb7f-6a6e-4170-a61d-2b1d76b42df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out_single_node(prefix, steps, step_id, answer, eval_task_type, metric_name, n_sample_per_node, node_join_char='\\n\\n'):\n",
    "    print(f'Roll out at (before) node {step_id}, {n_sample_per_node} samples per node...')\n",
    "\n",
    "    perf_dict = {}\n",
    "    \n",
    "    cur_prefix_nodes = steps[:step_id]\n",
    "    cur_prefix_text = prefix + node_join_char.join(cur_prefix_nodes)\n",
    "    print('Prompt:', [cur_prefix_text])\n",
    "\n",
    "    responses = client.completions.create(model=model_name, prompt=cur_prefix_text, n=n_sample_per_node, temperature=0.7, top_p=0.8, max_tokens=20000, timeout=OPENAI_REQUEST_TIMEOUT,\n",
    "                                          extra_body={'top_k': 20, 'include_stop_str_in_output': True, 'repetition_penalty': 1.05,})\n",
    "\n",
    "    sample_preds = [x.text for x in responses.choices]\n",
    "    sample_preds_processed = []\n",
    "    metrics = []\n",
    "    for cur_pred in sample_preds:\n",
    "        metrics_dict, processed_pred = evaluate_predictions(cur_pred, answer, mode=eval_task_type, use_llm=False, question=None, extract_answer=True)\n",
    "        sample_preds_processed.append(processed_pred)\n",
    "        metrics.append(metrics_dict[metric_name])\n",
    "    perf_dict[step_id] = {\n",
    "        'preds': sample_preds,\n",
    "        'preds_processed': sample_preds_processed,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    cur_node_mean_metrics = np.mean(metrics)\n",
    "    print(metrics)\n",
    "    print(f'Mean metrics {round(cur_node_mean_metrics, 4)}')\n",
    "    return perf_dict\n",
    "\n",
    "\n",
    "def roll_out_single_node_with_hint(orig_prefix, hint, steps, step_id, answer, eval_task_type, metric_name, n_sample_per_node, node_join_char='\\n\\n', max_tokens=24000):\n",
    "    \n",
    "    def process_question_instruct_add_hint(instruction):\n",
    "        if task in ['gpqa']:\n",
    "            raise NotImplementedError\n",
    "        elif task in ['aime', 'amc', 'math500']:\n",
    "            instruction_new = instruction.replace('Please answer the following math question.',\n",
    "                                                  'Please answer the following math question. Hints might be provided during your question answering wrapped within [hint] and [end of hint]. If you see hints, try to leverage them to guide your thinking process.')\n",
    "        elif task in ['livecode']:\n",
    "            raise NotImplementedError\n",
    "        elif task in ['bamboogle']:\n",
    "            instruction_new = instruction.replace('Please answer the following question.',\n",
    "                                                  'Please answer the following question. Hints might be provided during your question answering wrapped within [hint] and [end of hint]. If you see hints, try to leverage them to guide your thinking process.')\n",
    "        assert instruction_new != instruction\n",
    "        return instruction_new\n",
    "\n",
    "    print(f'Roll out with hint injected at (before) node {step_id}, {n_sample_per_node} samples per node...')\n",
    "\n",
    "    perf_dict = {}\n",
    "    \n",
    "    cur_prefix_nodes = steps[:step_id]\n",
    "    # cur_prefix_text = prefix + node_join_char.join(cur_prefix_nodes)\n",
    "    formatted_question = process_question_instruct_add_hint(orig_prefix)\n",
    "    hint_str = f'[hint] {hint} [end of hint]\\n\\nOkay,'\n",
    "    final_prompt = formatted_question + node_join_char.join(cur_prefix_nodes) + hint_str\n",
    "\n",
    "    print('Prompt:', [final_prompt])\n",
    "\n",
    "    responses = client.completions.create(model=model_name, prompt=final_prompt, n=n_sample_per_node, temperature=0.7, top_p=0.8, max_tokens=max_tokens, timeout=OPENAI_REQUEST_TIMEOUT,\n",
    "                                          extra_body={'top_k': 20, 'include_stop_str_in_output': True, 'repetition_penalty': 1.05,})\n",
    "\n",
    "    sample_preds = [x.text for x in responses.choices]\n",
    "    sample_preds_processed = []\n",
    "    metrics = []\n",
    "    for cur_pred in sample_preds:\n",
    "        metrics_dict, processed_pred = evaluate_predictions(cur_pred, answer, mode=eval_task_type, use_llm=False, question=None, extract_answer=True)\n",
    "        sample_preds_processed.append(processed_pred)\n",
    "        metrics.append(metrics_dict[metric_name])\n",
    "    perf_dict[step_id] = {\n",
    "        'preds': sample_preds,\n",
    "        'preds_processed': sample_preds_processed,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    cur_node_mean_metrics = np.mean(metrics)\n",
    "    print(metrics)\n",
    "    print(f'Mean metrics {round(cur_node_mean_metrics, 4)}')\n",
    "    return perf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97a3807e-7608-4cd9-acbc-850f0821caf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll out with hint injected at (before) node 0, 10 samples per node...\n",
      "Prompt: ['<|im_start|>user\\nPlease answer the following math question. Hints might be provided during your question answering wrapped within [hint] and [end of hint]. If you see hints, try to leverage them to guide your thinking process. You should provide your final answer in the format \\\\boxed{YOUR_ANSWER}.\\n\\nQuestion:\\n$\\\\overline{BC}$ is parallel to the segment through $A$, and $AB = BC$. What is the number of degrees represented by $x$?\\n\\n[asy]\\ndraw((0,0)--(10,0));\\ndraw((0,3)--(10,3));\\ndraw((2,3)--(8,0));\\ndraw((2,3)--(4,0));\\nlabel(\"$A$\",(2,3),N);\\nlabel(\"$B$\",(4,0),S);\\nlabel(\"$C$\",(8,0),S);\\nlabel(\"$124^{\\\\circ}$\",(2,3),SW);\\nlabel(\"$x^{\\\\circ}$\",(4.5,3),S);\\n[/asy]\\n\\n<|im_end|>\\n<|im_start|>assistant\\n<think>\\n[hint] When I\\'m faced with a geometry question that shows a triangle, tells me two lines are parallel, and states that two of the triangle’s sides are equal (making it isosceles), I remind myself to work in three moves. First, I translate any given angle from its exterior position into the triangle by using parallel-line rules (corresponding, alternate, or supplementary angles). Second, because the triangle is isosceles, I mark its two base angles as equal. Third, I apply the 180° interior-angle sum to relate the known angle(s) and the equal base angles, setting up a simple equation that lets me solve for the unknown. Ignoring the picture’s exact scale and trusting only the stated parallelism and side-length clues keeps me on track. [end of hint]\\n\\nOkay,']\n",
      "[False, False, False, False, False, False, False, False, False, False]\n",
      "Mean metrics 0.0\n",
      "0.0 [False, False, False, False, False, False, False, False, False, False]\n",
      "[\n",
      "    \"68\",\n",
      "    \"62\",\n",
      "    \"68\",\n",
      "    \"56\",\n",
      "    \"56\",\n",
      "    \"68\",\n",
      "    \"68\",\n",
      "    \"68\",\n",
      "    \"62\",\n",
      "    \"68\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = 1\n",
    "entry = orig_pred_logs_wrong[idx]\n",
    "question_only_prefix = entry['Question']\n",
    "answer = entry['answer']\n",
    "pred_steps = segment_thoughts_v2('\\n\\n'.join(entry['Output'].split('</think>')[:-1]))\n",
    "# len(pred_steps)\n",
    "# print(*pred_steps, sep='\\n=============\\n')\n",
    "# print(json.dumps(entry, indent=4))\n",
    "\n",
    "# step_to_inspect = 0\n",
    "# perf_dict_orig = roll_out_single_node(question_only_prefix, pred_steps, step_to_inspect, answer, eval_task_type, metric_name, n_sample_per_node=20, node_join_char='\\n\\n')\n",
    "# print(np.mean(perf_dict_orig[step_to_inspect]['metrics']), perf_dict[step_to_inspect]['metrics'])\n",
    "# print(json.dumps(perf_dict_orig[step_to_inspect]['preds_processed'], indent=4))\n",
    "# perf_dict_orig[step_to_inspect]['preds']\n",
    "\n",
    "\n",
    "step_to_intervene = 0\n",
    "# hint = \"\"\"Treat 124° as the exterior angle at A.\n",
    "# 1. First, find the interior angle at A by taking its supplement.\n",
    "# 2. Because AB = BC, triangle ABC is isosceles, so the interior angles at A and C are equal.\n",
    "# 3. Since BC is parallel to the line through A, the angle x on that top line corresponds to the interior angle at C.\"\"\"\n",
    "# hint = \"I should notice that BC is parallel to the line through A, so the marked 124° is an exterior angle of △ABC. First I’ll turn that into the interior angle at A, then remember that AB = BC makes △ABC isosceles, giving me equal base angles at A and C. Finally, I should use the 180° sum of the triangle’s interior angles to solve for x.\"\n",
    "# hint = (\n",
    "#     \"I should proceed in four clear steps and ignore the drawing’s scale:\\n\"\n",
    "#     \"1️⃣  Spot that the 124° lies between AB and the line through A that is **parallel to BC**. \"\n",
    "#     \"Because that line is an extension of AC’s direction, the 124° is an **exterior angle at A**.\\n\"\n",
    "#     \"2️⃣  Convert that exterior angle to the triangle’s interior angle at A by doing 180° – 124°.\\n\"\n",
    "#     \"3️⃣  Remember **AB = BC**, so △ABC is isosceles with vertex at B and base AC.  \"\n",
    "#     \"Therefore the two **base angles at A and C are equal**.\\n\"\n",
    "#     \"4️⃣  Use the triangle-angle-sum (A + B + C = 180°).  \"\n",
    "#     \"Once I know the interior angle at A, I immediately know the same angle at C; \"\n",
    "#     \"solving the sum then gives me the remaining apex angle, which corresponds to x.\"\n",
    "# )\n",
    "hint = (\n",
    "    \"When I'm faced with a geometry question that shows a triangle, tells me two lines are parallel, \"\n",
    "    \"and states that two of the triangle’s sides are equal (making it isosceles), I remind myself to work in three moves. \"\n",
    "    \"First, I translate any given angle from its exterior position into the triangle by using parallel-line rules \"\n",
    "    \"(corresponding, alternate, or supplementary angles). Second, because the triangle is isosceles, I mark its two base angles as equal. \"\n",
    "    \"Third, I apply the 180° interior-angle sum to relate the known angle(s) and the equal base angles, setting up a simple equation \"\n",
    "    \"that lets me solve for the unknown. Ignoring the picture’s exact scale and trusting only the stated parallelism and side-length clues \"\n",
    "    \"keeps me on track.\"\n",
    ")\n",
    "hint = hint.strip()\n",
    "perf_dict_intervene = roll_out_single_node_with_hint(question_only_prefix, hint, pred_steps, step_to_intervene, answer, \n",
    "                                                     eval_task_type, metric_name, n_sample_per_node=10, node_join_char='\\n\\n', max_tokens=20000)\n",
    "print(np.mean(perf_dict_intervene[step_to_intervene]['metrics']), perf_dict_intervene[step_to_intervene]['metrics'])\n",
    "print(json.dumps(perf_dict_intervene[step_to_intervene]['preds_processed'], indent=4))\n",
    "# print(json.dumps(perf_dict_intervene[step_to_intervene]['preds'], indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1815a-1965-47d7-8cda-ac988f2cf081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03675c9e-6ac2-4d60-a41d-7527858f5df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114c2ad-97fb-4c65-8e42-dd10f4e2d9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a8e54-2b06-474e-af6e-eda2ef2ee5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff63e2-ffa4-4960-bd94-4c299c776c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985121a2-3b3e-4a07-9897-362774e75bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
